# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12GqGAjmeuNkk5aJSXJOuj3GNzYTNTOp1
"""

#Cambio para implementar RAG
from PyPDF2 import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

#

import streamlit as st
import os
import time
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from typing import List, Dict
from datetime import datetime
from openai import OpenAI

# =============================================================================
# PAGE CONFIGURATION
# =============================================================================

st.set_page_config(
    page_title="Trustworthy AI Explainer",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =============================================================================
# LLM CONFIGURATION
# =============================================================================

@st.cache_resource
def load_llm():
    """
    Load OpenAI client (cached).
    Requires OPENAI_API_KEY in environment variables.
    """
    return OpenAI()

# =============================================================================
# CACHED DATA FUNCTIONS
# =============================================================================

@st.cache_data(ttl=3600)
def load_feedback_data() -> pd.DataFrame:
    if 'feedback_db' in st.session_state:
        return pd.DataFrame(st.session_state.feedback_db)

    return pd.DataFrame(
        columns=["timestamp", "message", "response", "rating", "comment"]
    )

@st.cache_data
def compute_explanation(_input_text: str, _response: str) -> Dict:
    """
    Explainability placeholder (SHAP / LIME hook).
    """
    return {
        "input_tokens": len(_input_text.split()),
        "response_tokens": len(_response.split()),
        "confidence": 0.85,
        "top_features": [
            "Input length",
            "Semantic similarity",
            "Context relevance",
            "Prompt clarity"
        ],
        "explanation": "Replace with SHAP / LIME analysis"
    }

# =============================================================================
# SESSION STATE
# =============================================================================

def initialize_session_state():

    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "feedback_db" not in st.session_state:
        st.session_state.feedback_db = []

    if "preferences" not in st.session_state:
        st.session_state.preferences = {
            "temperature": 0.7,
            "max_tokens": 500,
            "system_prompt": "You are a helpful, transparent AI assistant."
        }

    if "current_explanation" not in st.session_state:
        st.session_state.current_explanation = None

    if "metrics" not in st.session_state:
        st.session_state.metrics = {
            "total_messages": 0,
            "avg_response_time": 0.0,
            "total_feedback": 0
        }
if "vectordb" not in st.session_state:
    st.session_state.vectordb = None

# =============================================================================
# CORE LOGIC
# =============================================================================

def generate_response(message: str, temperature: float):
    start = time.time()
    client = load_llm()

    if st.session_state.vectordb is None:
        return (
            "Please upload a PDF document first.",
            None,
            0.0
        )

    retriever = st.session_state.vectordb.as_retriever(
        search_kwargs={"k": 4}
    )

    docs = retriever.invoke(message)

    if not docs:
        context = ""
    else:
        context = "\n\n".join(d.page_content for d in docs)

    system_prompt = (
        "You are a strict AI assistant.\n"
        "Answer ONLY using the context provided.\n"
        "If the answer is not in the context, say:\n"
        "'I cannot find that information in the provided document.'"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": f"Context:\n{context}\n\nQuestion:\n{message}"
        }
    ]

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=temperature,
        max_tokens=st.session_state.preferences["max_tokens"]
    )

    output = response.choices[0].message.content
    explanation = compute_explanation(message, output)
    elapsed = time.time() - start

    return output, explanation, elapsed

#Codigo para que pida pdf
@st.cache_resource
def process_pdf(uploaded_file):
    reader = PdfReader(uploaded_file)

    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )

    docs = splitter.create_documents([text])

    embeddings = OpenAIEmbeddings()
    vectordb = Chroma.from_documents(docs, embeddings)

    return vectordb

#

def save_feedback(message, response, rating, comment):
    st.session_state.feedback_db.append({
        "timestamp": datetime.now(),
        "message": message,
        "response": response,
        "rating": rating,
        "comment": comment
    })
    st.session_state.metrics["total_feedback"] += 1
    load_feedback_data.clear()

# =============================================================================
# PAGE: CHAT
# =============================================================================

def page_chat():
    st.title("ğŸ’¬ AI Chat with Explainability")

    with st.sidebar:
        st.header("âš™ï¸ Settings")

        st.session_state.preferences["temperature"] = st.slider(
            "Temperature", 0.0, 2.0,
            st.session_state.preferences["temperature"], 0.1
        )

        st.session_state.preferences["max_tokens"] = st.slider(
            "Max tokens", 50, 2000,
            st.session_state.preferences["max_tokens"], 50
        )

        st.session_state.preferences["system_prompt"] = st.text_area(
            "System Prompt",
            st.session_state.preferences["system_prompt"],
            height=120
        )
        st.divider()
        st.header("ğŸ“„ RAG Document")

        uploaded_file = st.file_uploader(
            "Upload a PDF",
            type=["pdf"]
        )

        if uploaded_file:
            with st.spinner("Processing document..."):
                st.session_state.vectordb = process_pdf(uploaded_file)

            st.success("Document indexed successfully")

        if st.button("ğŸ—‘ï¸ Clear Chat"):
            st.session_state.messages = []
            st.session_state.current_explanation = None
            st.rerun()

    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("Conversation")

        chat_box = st.container(height=420)
        with chat_box:
            for msg in st.session_state.messages:
                with st.chat_message(msg["role"]):
                    st.markdown(msg["content"])

        if prompt := st.chat_input("Type your message..."):
            st.session_state.messages.append({"role": "user", "content": prompt})

            with chat_box:
                with st.chat_message("user"):
                    st.markdown(prompt)

            with st.spinner("Thinking..."):
                response, explanation, rt = generate_response(
                    prompt,
                    st.session_state.preferences["temperature"]
                )

            st.session_state.messages.append(
                {"role": "assistant", "content": response}
            )

            st.session_state.current_explanation = {
                "input": prompt,
                "output": response,
                "details": explanation,
                "response_time": rt
            }

            with chat_box:
                with st.chat_message("assistant"):
                    st.markdown(response)

            st.rerun()

    with col2:
        st.subheader("Explainability")

        if st.session_state.current_explanation:
            exp = st.session_state.current_explanation

            st.metric("Confidence", f"{exp['details']['confidence']:.2f}")
            st.metric("Response Time", f"{exp['response_time']:.2f}s")

            st.markdown("**Key Factors:**")
            for f in exp["details"]["top_features"]:
                st.markdown(f"- {f}")

            st.divider()
            rating = st.radio("Rate response", ["ğŸ‘ Helpful", "ğŸ‘ Not Helpful"])
            comment = st.text_area("Comment (optional)")

            if st.button("Submit Feedback"):
                save_feedback(
                    exp["input"],
                    exp["output"],
                    rating,
                    comment
                )
                st.success("Feedback saved!")
        else:
            st.info("Send a message to see explainability.")

# =============================================================================
# OTHER PAGES (reuse yours unchanged)
# =============================================================================

def page_explainability():
    st.title("ğŸ” Explainability Analysis")
    st.info("Reuse your existing implementation here (no changes needed).")

def page_feedback():
    st.title("ğŸ“Š Feedback Dashboard")
    df = load_feedback_data()
    if df.empty:
        st.info("No feedback yet.")
        return
    st.dataframe(df, use_container_width=True)

def page_monitoring():
    st.title("ğŸ“ˆ Monitoring")
    m = st.session_state.metrics
    st.metric("Total Messages", m["total_messages"])
    st.metric("Avg Response Time", f"{m['avg_response_time']:.2f}s")
    st.metric("Total Feedback", m["total_feedback"])

def page_documentation():
    st.title("ğŸ“š Documentation")
    st.markdown("Trustworthy AI Explainer â€“ Module 15")

# =============================================================================
# MAIN
# =============================================================================

def main():
    initialize_session_state()

    with st.sidebar:
        page = st.radio(
            "Navigation",
            ["ğŸ’¬ Chat", "ğŸ” Explainability", "ğŸ“Š Feedback", "ğŸ“ˆ Monitoring", "ğŸ“š Documentation"]
        )

    if page == "ğŸ’¬ Chat":
        page_chat()
    elif page == "ğŸ” Explainability":
        page_explainability()
    elif page == "ğŸ“Š Feedback":
        page_feedback()
    elif page == "ğŸ“ˆ Monitoring":
        page_monitoring()
    elif page == "ğŸ“š Documentation":
        page_documentation()

if __name__ == "__main__":
    main()
